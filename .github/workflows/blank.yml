name: Ollama with Persistent Ngrok Tunnel

on:
  workflow_dispatch:  # Allows manual execution

jobs:
  run-ollama-ngrok:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          echo "Ollama installed successfully."

      - name: Start Ollama Daemon
        run: |
          nohup ollama serve > ollama.log 2>&1 &
          sleep 5  # Allow Ollama to initialize

      - name: Download LLM model
        run: |
          ollama pull qwen:0.5b  # Change to your preferred model
          echo "Model downloaded successfully."
          ollama run qwen:0.5b

      - name: Download & Install Ngrok
        run: |
          curl -sSL https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -o ngrok.zip
          unzip ngrok.zip
          chmod +x ngrok
          mv ngrok /usr/local/bin/ngrok
          echo "âœ… Ngrok installed."

      - name: Authenticate Ngrok
        run: |
          ngrok authtoken $NGROK_AUTH_TOKEN
        env:
          NGROK_AUTH_TOKEN: ${{ secrets.NGROK_AUTH_TOKEN }}

      - name: Start Ngrok Tunnel
        run: |
          nohup ngrok http 11434 > ngrok.log 2>&1 &
          sleep 15

      - name: Fetch Ngrok URL (with retries)
        run: |
          echo "â³ Waiting for Ngrok to provide a public URL..."
          for i in {1..20}; do
            NGROK_URL=$(curl -s http://localhost:4040/api/tunnels | jq -r '.tunnels[0].public_url')
            if [[ "$NGROK_URL" != "null" && "$NGROK_URL" != "" ]]; then
              echo "âœ… Ollama LLM is running at: $NGROK_URL"
              echo "NGROK_URL=$NGROK_URL" >> $GITHUB_ENV
              exit 0
            fi
            sleep 5
          done
          echo "âŒ ERROR: Ngrok did not return a public URL."
          exit 1

      - name: Keep Action Alive
        run: |
          echo "ğŸ”„ Keeping the server alive..."
          tail -f /dev/null
